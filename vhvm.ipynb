{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f8d333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d86110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80b782",
   "metadata": {},
   "source": [
    "### Download SP500 assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab49370",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sectors_dict.pickle', 'rb') as f:\n",
    "    sectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4436ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIRNAME = '../stmom/SpatioTemporalMomentum/yf_data'\n",
    "PERIOD = '1day'\n",
    "START_DATE = '2010-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe101cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DATASET_DIRNAME):\n",
    "    os.mkdir(DATASET_DIRNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa018d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in tqdm(sectors.keys()):\n",
    "    if not os.path.exists(os.path.join(DATASET_DIRNAME, key)):\n",
    "        os.mkdir(os.path.join(DATASET_DIRNAME, key))\n",
    "    for symbol in sectors[key]:\n",
    "        data = yf.download(symbol, period=PERIOD, start=START_DATE)\n",
    "        data.to_csv(os.path.join(DATASET_DIRNAME, key, '{}.csv'.format(symbol)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486b873",
   "metadata": {},
   "source": [
    "### Load data of randomly chosen assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6883690",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_ASSETS_PER_SECTOR = 5\n",
    "LENGTH_THRESHOLD = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1446d39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataframe shape (3068, 56)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "all_dfs = []\n",
    "for current_sector in sectors.keys():\n",
    "    dfs = []\n",
    "    for asset in sectors[current_sector]:\n",
    "        df = pd.read_csv(os.path.join(DATASET_DIRNAME, f'{current_sector}', f'{asset}.csv'))\n",
    "        # use only stocks with long enough history\n",
    "        if len(df) > LENGTH_THRESHOLD:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "            df = df[['Date', 'Adj Close']]\n",
    "            df.columns = ['Date', asset]\n",
    "            dfs.append(df)\n",
    "\n",
    "    df = dfs[0]\n",
    "    for i in range(1, len(dfs)):\n",
    "        df = pd.merge(df, dfs[i], left_on='Date', right_on='Date', how='inner')\n",
    "\n",
    "    assets = list(df.columns[1:])\n",
    "    used_assets = np.random.choice(assets, N_ASSETS_PER_SECTOR, replace=False)\n",
    "    df = df[['Date'] + list(used_assets)]\n",
    "    all_dfs.append(df)\n",
    "\n",
    "df = all_dfs[0]\n",
    "for i in range(1, len(all_dfs)):\n",
    "    df = pd.merge(df, all_dfs[i], left_on='Date', right_on='Date', how='inner')\n",
    "df = df.set_index(df['Date'])\n",
    "print('loaded dataframe shape', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ab8e9",
   "metadata": {},
   "source": [
    "### Calculate log returns, center its mean to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e5393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assets = list(df.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13050c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of log returns dataframe 2.714924253240519e-18\n"
     ]
    }
   ],
   "source": [
    "# create log returns\n",
    "log_returns_df = np.log(df[all_assets]) - np.log(df[all_assets].shift(1))\n",
    "log_returns_df = log_returns_df.iloc[1:]\n",
    "log_returns_mean = log_returns_df.mean(axis=0)\n",
    "log_returns_df = log_returns_df - log_returns_mean\n",
    "print('Mean of log returns dataframe', log_returns_df.mean().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f117a7",
   "metadata": {},
   "source": [
    "### Split data set into train/validation/test and format it as tensors of shape (n_samples, n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b62ecca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start = pd.Timestamp('2020-01-01')\n",
    "validation_end = pd.Timestamp('2021-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9edf9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(len(log_returns_df))\n",
    "log_returns_df.loc[:, 'index'] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ca5dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxes = log_returns_df.loc[:validation_start - pd.Timedelta('1day'), 'index'].values\n",
    "val_idxes = log_returns_df.loc[validation_start:validation_end, 'index'].values\n",
    "test_idxes = log_returns_df.loc[validation_end + pd.Timedelta('1day'):, 'index'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef367fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_returns_df = log_returns_df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cf2a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try window size of 3 weeks\n",
    "history_size = 21*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c730ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(log_returns_df), history_size, log_returns_df.shape[1]))\n",
    "for i, col in enumerate(log_returns_df.columns):\n",
    "    for j in range(history_size):\n",
    "        X[:, j, i] = log_returns_df[col].shift(history_size - j - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa5769b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[train_idxes]\n",
    "X_val = X[val_idxes]\n",
    "X_test = X[test_idxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "733f84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[history_size:]\n",
    "train_idxes = train_idxes[history_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa117416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "X_test = torch.Tensor(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e8a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# fix random seed of train dataloader for reproducibility\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_loader = DataLoader(TensorDataset(X_train, X_train), shuffle=True, batch_size=128)\n",
    "val_loader = DataLoader(TensorDataset(X_val, X_val), shuffle=False, batch_size=128)\n",
    "test_loader = DataLoader(TensorDataset(X_test, X_test), shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f78d38",
   "metadata": {},
   "source": [
    "### Setup model, optimizer and learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f70a0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VRNN(torch.jit.ScriptModule):\n",
    "    def __init__(self, input_dim, hidden_dim=128, device='cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = int(input_dim*(input_dim+1)/2)\n",
    "        self.N = input_dim\n",
    "        self.gru_cell = nn.GRUCell(input_dim, hidden_dim)\n",
    "        self.mlp_gen = nn.Sequential(nn.Linear(hidden_dim, 2*hidden_dim),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Linear(2*hidden_dim, hidden_dim))\n",
    "        self.mlp_gen_mu = nn.Linear(hidden_dim, self.latent_dim)\n",
    "        self.mlp_gen_sigma = nn.Sequential(nn.Linear(hidden_dim, self.latent_dim), nn.Softplus())\n",
    "        \n",
    "        self.mlp_inf = nn.Sequential(nn.Linear(hidden_dim, 2*hidden_dim),\n",
    "                                     nn.LeakyReLU(),\n",
    "                                     nn.Linear(2*hidden_dim, hidden_dim))\n",
    "        self.mlp_inf_mu = nn.Linear(hidden_dim, self.latent_dim)\n",
    "        self.mlp_inf_sigma = nn.Sequential(nn.Linear(hidden_dim, self.latent_dim), nn.Softplus())\n",
    "        \n",
    "        self.softplus = nn.Softplus()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward(self, r):\n",
    "        h_tm1 = torch.zeros(r.shape[0], self.hidden_dim).to(self.device)\n",
    "        loglike_loss = torch.tensor(0.0).to(self.device)\n",
    "        kl_loss = torch.tensor(0.0).to(self.device)\n",
    "        Ls = []\n",
    "        \n",
    "        for t in range(r.shape[1]):\n",
    "            \n",
    "            # calculate prior P(z_t|h_tm1)\n",
    "            prior = self.mlp_gen(h_tm1)\n",
    "            prior_mu = self.mlp_gen_mu(prior)\n",
    "            prior_sigma = self.mlp_gen_sigma(prior)\n",
    "            \n",
    "            # calculate hidden state transition h_t = f(r_t, h_tm1)\n",
    "            h_t = self.gru_cell(r[:, t, :], h_tm1)\n",
    "            \n",
    "            # calculate posterior q(z_t|h_t)\n",
    "            encoded = self.mlp_inf(h_t)\n",
    "            encoded_mu = self.mlp_inf_mu(encoded)\n",
    "            encoded_sigma = self.mlp_inf_sigma(encoded)\n",
    "            \n",
    "            random_noise = torch.randn_like(prior_mu).to(self.device)\n",
    "            z_t = random_noise*prior_sigma + prior_mu\n",
    "            \n",
    "            loglike_loss_, L_t = self._loglike_loss(z_t, r[:, t, :])\n",
    "            loglike_loss += loglike_loss_\n",
    "            kl_loss += self._kl_loss(encoded_mu, encoded_sigma, prior_mu, prior_sigma)\n",
    "            \n",
    "            h_tm1 = h_t\n",
    "            Ls.append(L_t)\n",
    "            \n",
    "        return loglike_loss, kl_loss, Ls\n",
    "\n",
    "    \n",
    "    def _construct_choleskiy_matrix(self, z_t):\n",
    "       \n",
    "        L_t = torch.zeros(z_t.shape[0], self.N, self.N).to(self.device)\n",
    "        idxes = torch.tril_indices(self.N, self.N)\n",
    "        L_t[:, idxes[0], idxes[1]] = z_t\n",
    "        \n",
    "        # ensure positive definiteness\n",
    "        diag_idxes = torch.arange(self.N)\n",
    "        diag = L_t[:, diag_idxes, diag_idxes]\n",
    "        L_t[:, diag_idxes, diag_idxes] = 0\n",
    "        diag = self.softplus(diag)\n",
    "        L_t[:, diag_idxes, diag_idxes] += diag\n",
    "        \n",
    "        return L_t, diag\n",
    "    \n",
    "    \n",
    "    def _loglike_loss(self, z_t, r_t, eps: float=1e-9):\n",
    "        \n",
    "        L_t, diag = self._construct_choleskiy_matrix(z_t)\n",
    "        \n",
    "        # evaluate log(det(sigma_t))\n",
    "        log_det = -2*torch.sum(torch.log(diag + eps), dim=1)\n",
    "        \n",
    "        # calculate Mahalanobis distance\n",
    "        r_t = r_t.unsqueeze(2)\n",
    "        r_T_t = r_t.permute(0, 2, 1)\n",
    "        L_T_t = torch.transpose(L_t, 1, 2)\n",
    "        \n",
    "        mach_distance = r_T_t @ L_t @ L_T_t @ r_t\n",
    "        \n",
    "        #squeeze extra dimensions to avoid broadcasting\n",
    "        mach_distance = mach_distance.squeeze(1).squeeze(1)\n",
    "        \n",
    "        loglike_loss = -1/2.0*(log_det + mach_distance)\n",
    "        loglike_loss = torch.mean(loglike_loss)\n",
    "        \n",
    "        return loglike_loss, L_t\n",
    "    \n",
    "    \n",
    "    def _kl_loss(self, mean_1, sigma_1, mean_2, sigma_2, eps: float=1e-9):\n",
    "\n",
    "        kld_element =  (2 * torch.log(sigma_2 + eps) - 2 * torch.log(sigma_1 + eps) + \n",
    "            (sigma_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "            sigma_2.pow(2) - 1)\n",
    "        return 0.5 * torch.sum(kld_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "031d2614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _set_seed(seed) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    #torch.backends.cudnn.deterministic = True\n",
    "    #torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "    \n",
    "device = 'cpu'\n",
    "step = 10\n",
    "gamma = 0.8\n",
    "early_stopping_rounds = 15\n",
    "n_epochs = 100\n",
    "kl_coef = 1\n",
    "best_val_loss = np.PINF\n",
    "\n",
    "# fix random seed of model's weights initialization for reproducibility\n",
    "_set_seed(SEED)\n",
    "model = VRNN(X_train.shape[2]).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608a287",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6315f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "train_loglike_losses = []\n",
    "val_loglike_losses = []\n",
    "train_kl_losses = []\n",
    "val_kl_losses = []\n",
    "\n",
    "for e in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    train_loglike_loss_epoch = 0\n",
    "    train_kl_loss_epoch = 0\n",
    "    for batch_x, _ in tqdm(train_loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loglike_loss, kl_loss, _ = model(batch_x)\n",
    "        \n",
    "        # we seek to maximise the multivariate normal log likelihood function, so we multiply loglike_loss by -1\n",
    "        # kl_coef is used to control tradeoff between losses\n",
    "        loss = -1*loglike_loss + kl_coef*kl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        train_loglike_loss_epoch += loglike_loss.item()\n",
    "        train_kl_loss_epoch += kl_loss.item()\n",
    "        \n",
    "    train_loglike_losses.append(train_loglike_loss_epoch/len(train_loader))\n",
    "    train_kl_losses.append(train_kl_loss_epoch/len(train_loader))\n",
    "    \n",
    "    # if lr is small enough, do not decrease it\n",
    "    if scheduler.get_last_lr()[0] > 1e-5:\n",
    "        scheduler.step()\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_loglike_loss_epoch = 0\n",
    "    val_kl_loss_epoch = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in tqdm(val_loader):\n",
    "            batch_x = batch_x.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loglike_loss, kl_loss, _ = model(batch_x)\n",
    "\n",
    "            loss = -1*loglike_loss + kl_coef*kl_loss\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_loglike_loss_epoch += loglike_loss.item()\n",
    "            val_kl_loss_epoch += kl_loss.item()\n",
    "            \n",
    "    val_loglike_losses.append(val_loglike_loss_epoch/len(val_loader))\n",
    "    val_kl_losses.append(val_kl_loss_epoch/len(val_loader))\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    if np.sign(val_loss) != np.sign(best_val_loss):\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    # save best model's weights \n",
    "    if best_val_loss > val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), 'vrnn_zero_mean.pt')\n",
    "    else:\n",
    "        counter += 1\n",
    "    \n",
    "    # if validation loss doesn't improve for several epochs, stop training\n",
    "    if counter > early_stopping_rounds:\n",
    "        break\n",
    "    \n",
    "    print('Iter: ', e,\n",
    "          'train loglike loss: ', round(train_loglike_losses[-1], 3),\n",
    "          'train kl loss: ', round(train_kl_losses[-1], 3),\n",
    "          'val loglike loss: ', round(val_loglike_losses[-1], 3),\n",
    "          'val kl loss: ', round(val_kl_losses[-1], 3),\n",
    "          'total val loss: ', round(val_loss, 3),\n",
    "          'best val loss: ', round(best_val_loss, 3),\n",
    "          'epochs till end: ', early_stopping_rounds - counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8621696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model's weights\n",
    "model.load_state_dict(torch.load('vrnn_zero_mean.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1f9ed8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c72d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Log Likelihood loss')\n",
    "plt.plot(train_loglike_losses, label='train', marker='o')\n",
    "plt.plot(val_loglike_losses, label='validation', marker='o')\n",
    "plt.ylabel('Log Likelihood')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e79619",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('KL loss')\n",
    "plt.plot(train_kl_losses, label='train', marker='o')\n",
    "plt.plot(val_kl_losses, label='validation', marker='o')\n",
    "plt.ylabel('KL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db5045e",
   "metadata": {},
   "source": [
    "### Evaluate model performance in terms of  multivariate normal log likelihood function (higher is better)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28202abe",
   "metadata": {},
   "source": [
    "#### (Note that inference is stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "debe8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglike_loss_fn(sigma, r_t):\n",
    "    r_t = r_t[:, np.newaxis]\n",
    "    r_T_t = np.swapaxes(r_t, 0, 1)\n",
    "    P = np.linalg.inv(sigma)\n",
    "    \n",
    "    mach_distance = r_T_t @ P @ r_t\n",
    "    mach_distance = mach_distance[0, 0]\n",
    "    \n",
    "    log_det = np.log(np.linalg.det(sigma) + 1e-9)\n",
    "\n",
    "    loss = log_det + mach_distance\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "448b65ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165c03a6807846ffa8c4a2f718657484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHVM validation loss 351.4297228599578\n"
     ]
    }
   ],
   "source": [
    "val_sigmas = []\n",
    "val_losses = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, _ in tqdm(val_loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loglike_loss, kl_loss, Ls = model(batch_x)\n",
    "        \n",
    "        # use last timestamp as we no longer need time dimension\n",
    "        L_t = Ls[-1].numpy()\n",
    "        r_t = batch_x[:, -1, :].numpy()\n",
    "        L_T_t = np.transpose(L_t, axes=(0, 2, 1))\n",
    "        \n",
    "        for i in range(len(r_t)):\n",
    "            # calculate precision matrix\n",
    "            P = L_t[i] @ L_T_t[i]\n",
    "            # calculate covariance matrix\n",
    "            sigma = np.linalg.inv(P)\n",
    "        \n",
    "            val_loss = loglike_loss_fn(sigma, r_t[i])\n",
    "            val_sigmas.append(sigma)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "# ensure that calculated covariance matrix is symmetric and positive definite\n",
    "for j in range(len(val_sigmas)):\n",
    "    assert scipy.linalg.issymmetric(val_sigmas[j], atol=1e-8)\n",
    "    assert np.all(np.linalg.eigvals(val_sigmas[j]))\n",
    "\n",
    "val_losses = np.array(val_losses)\n",
    "val_loglike_loss = -1/2.0*np.sum(val_losses)\n",
    "print('VHVM validation loss', val_loglike_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22b35a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21060d136d1846d28b25e780a637febb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VHVM test loss 4878.875368932642\n"
     ]
    }
   ],
   "source": [
    "test_sigmas = []\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for batch_x, _ in tqdm(test_loader):\n",
    "        batch_x = batch_x.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        loglike_loss, kl_loss, Ls = model(batch_x)\n",
    "        \n",
    "        # use last timestamp as we no longer need time dimension\n",
    "        L_t = Ls[-1].numpy()\n",
    "        r_t = batch_x[:, -1, :].numpy()\n",
    "        L_T_t = np.transpose(L_t, axes=(0, 2, 1))\n",
    "        \n",
    "        for i in range(len(r_t)):\n",
    "            # calculate precision matrix\n",
    "            P = L_t[i] @ L_T_t[i]\n",
    "            # calculate covariance matrix\n",
    "            sigma = np.linalg.inv(P)\n",
    "        \n",
    "            test_loss = loglike_loss_fn(sigma, r_t[i])\n",
    "            test_sigmas.append(sigma)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "# ensure that calculated covariance matrix is symmetric and positive definite\n",
    "for j in range(len(test_sigmas)):\n",
    "    assert scipy.linalg.issymmetric(test_sigmas[j], atol=1e-8)\n",
    "    assert np.all(np.linalg.eigvals(test_sigmas[j]))\n",
    "\n",
    "test_losses = np.array(test_losses)\n",
    "test_loglike_loss = -1/2.0*np.sum(test_losses)\n",
    "print('VHVM test loss', test_loglike_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd3639",
   "metadata": {},
   "source": [
    "### Lets compare VHVM approach with Ledoit Wolf covariance estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1725a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import LedoitWolf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9172b392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356f6d33dfa94059bce5abb96baaf6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LedoitWolf validation loss -13443.012589966645\n"
     ]
    }
   ],
   "source": [
    "val_sigmas_lw = []\n",
    "val_losses_lw = []\n",
    "\n",
    "idx_start = len(log_returns_df.loc[:validation_start])\n",
    "idx_end = len(log_returns_df.loc[:validation_end])\n",
    "\n",
    "for idx in tqdm(range(idx_start, idx_end)):\n",
    "    df_slice = log_returns_df.iloc[:idx]\n",
    "    r_t = df_slice.values[-1]\n",
    "\n",
    "    lw = LedoitWolf()\n",
    "    \n",
    "    sigma_lw = lw.fit(df_slice.values).covariance_\n",
    "    \n",
    "    loss_lw = loglike_loss_fn(sigma_lw, r_t)\n",
    "    \n",
    "    val_sigmas_lw.append(sigma_lw)\n",
    "    val_losses_lw.append(loss_lw)\n",
    "\n",
    "\n",
    "val_losses_lw = np.array(val_losses_lw)\n",
    "val_loss_lw = -1/2.0*np.sum(val_losses_lw)\n",
    "print('LedoitWolf validation loss', val_loss_lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbe0a66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a197ee9958934bd88a93b1431fa3162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LedoitWolf test loss -13256.787454213252\n"
     ]
    }
   ],
   "source": [
    "test_sigmas_lw = []\n",
    "test_losses_lw = []\n",
    "\n",
    "idx_start = len(log_returns_df.loc[:validation_end+pd.Timedelta('1day')])\n",
    "idx_end = len(log_returns_df)\n",
    "\n",
    "for idx in tqdm(range(idx_start, idx_end)):\n",
    "    df_slice = log_returns_df.iloc[:idx]\n",
    "    r_t = df_slice.values[-1]\n",
    "\n",
    "    lw = LedoitWolf()\n",
    "    \n",
    "    sigma_lw = lw.fit(df_slice.values).covariance_\n",
    "    \n",
    "    loss_lw = loglike_loss_fn(sigma_lw, r_t)\n",
    "    \n",
    "    test_sigmas_lw.append(sigma_lw)\n",
    "    test_losses_lw.append(loss_lw)\n",
    "\n",
    "\n",
    "test_losses_lw = np.array(test_losses_lw)\n",
    "test_loss_lw = -1/2.0*np.sum(test_losses_lw)\n",
    "print('LedoitWolf test loss', test_loss_lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42abff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
